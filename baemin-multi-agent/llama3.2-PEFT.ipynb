{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9de7f0a4-8788-464f-8e30-65a0b65e3991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os \n",
    "os.environ[\"HF_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "459e6c86-70cf-4627-b001-ae5d8d0fa448",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/freenak/miniconda3/envs/peft_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|█████████████████████████████| 2/2 [00:14<00:00,  7.08s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import json\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import pipeline\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    load_in_8bit=True,\n",
    "    #device_map=\"cuda\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# check https://github.com/huggingface/peft/blob/main/examples/int8_training/Finetune_opt_bnb_peft.ipynb\n",
    "from peft import prepare_model_for_kbit_training\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c8a0ebf-f827-459f-8cb5-c87407c443cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a0bca38-8fc2-42bb-adb0-166cc8fd22ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "  param.requires_grad = False  # freeze the model - train adapters later\n",
    "  if param.ndim == 1:\n",
    "    param.data = param.data.to(torch.float16)\n",
    "\n",
    "model.gradient_checkpointing_enable()  # reduce number of stored activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "278857ef-c9ee-4a99-95e6-feadb438608f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params:,} || all params: {all_param:,} || trainable : {100 * trainable_params / all_param}%\"\n",
    "    )\n",
    "    return trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d91bf9e1-549b-451e-8184-758ff23a26d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 0 || all params: 3,212,749,824 || trainable : 0.0%\n"
     ]
    }
   ],
   "source": [
    "ori_p = print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b742f3a5-1b38-48c5-9668-76038585c034",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edfe2585-c5a3-4225-adf5-4f198838a170",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=16, #attention heads\n",
    "    lora_alpha=32, #alpha scaling\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\" # set this for CAUSAL LANGUAGE MODELS (like Bloom, LLaMA) or SEQ TO SEQ (like FLAN, T5)\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "066e6c7b-1c9d-489d-8124-99eaebcbe621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,587,520 || all params: 3,217,337,344 || trainable : 0.1425874724810952%\n"
     ]
    }
   ],
   "source": [
    "peft_p = print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d16e4797-cbde-46e7-9a4d-6b7789e9a774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"kimjaewon/baemin_sft_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "739bb674-2e89-4815-bb67-29343b149521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'positive_document_list', 'negative_document_list', 'answer'],\n",
       "        num_rows: 1452\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1dd7b5c4-3fdf-4355-a3c9-27adcc2fdd30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 1452\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_dataset = dataset.remove_columns(['positive_document_list', 'negative_document_list'])\n",
    "peft_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54440e3c-774a-4913-822a-a14e50d3d284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['정산이 이뤄지는 시점은 언제인가요?',\n",
       " '주문 차단을 하고 싶다면 어떻게 해야 하나요?',\n",
       " '탈퇴 회원의 댓글 작성자 본인 여부를 확인할 수 있는 방법은 없나요?',\n",
       " '네이버 플레이스에 가게 정보를 제공하면 연동 여부 반영까지 얼마나 걸리나요?',\n",
       " '어떤 음식은 배달의민족을 통해 판매할 수 없나요?',\n",
       " '어떤 경우에 고객센터로 문의해야 하나요?',\n",
       " '울트라콜과 오픈리스트 상품의 배달팁은 어떻게 구분되나요?',\n",
       " '배민라이더스는 배민셀프서비스를 통한 배달지역 수정이 가능한가요?']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_dataset['train']['question'][:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9d401d2-5209-4e41-8f19-3df87543c4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████████████████████| 1452/1452 [00:00<00:00, 14529.16 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['어떤 경우에 고객센터로 문의해야 하나요? [배민 데이터 참조] 고객의 개인정보와 관련된 문의와 함께 배달의 불만사항, 결제문제 등 배달 서비스와 관련된 문제가 발생한 경우, 그리고 배달 관련 추가 연락이 필요한 경우에는 고객센터로 문의해주시면 최대한 도움을 드리겠습니다.',\n",
       " '울트라콜과 오픈리스트 상품의 배달팁은 어떻게 구분되나요? [배민 데이터 참조] 울트라콜과 오픈리스트 상품의 배달팁은 용도에 따라 기본 배달팁과 할증 배달팁으로 나누어집니다. 기본 배달팁은 주문금액에 따른 최대 3개의 배달팁 설정이 가능하며, 할증 배달팁은 지역, 시간대, 공휴일 등을 위해 별도로 설정이 가능합니다.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def merge_cols(example):\n",
    "    example[\"prediction\"] = example[\"question\"] + \" [배민 데이터 참조] \" + example[\"answer\"]\n",
    "    return example\n",
    "\n",
    "peft_dataset['train'] = peft_dataset['train'].map(merge_cols) # <-- 모든 문장에 대해 처리해 줍니다.\n",
    "peft_dataset['train'][\"prediction\"][5:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78a5e1fd-3594-4b40-83bd-1b3df1ef4cea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': '정산이 이뤄지는 시점은 언제인가요?',\n",
       " 'answer': '매출이 발생된 기점(D일)으로부터 영업일 기준 D+3일 이후 정산되어 지급됩니다.',\n",
       " 'prediction': '정산이 이뤄지는 시점은 언제인가요? [배민 데이터 참조] 매출이 발생된 기점(D일)으로부터 영업일 기준 D+3일 이후 정산되어 지급됩니다.'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf10b97c-a022-4d4f-82bd-7ba66fd938e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████████████████| 1452/1452 [00:00<00:00, 7474.44 examples/s]\n"
     ]
    }
   ],
   "source": [
    "peft_dataset = peft_dataset.map(\n",
    "                        lambda x: tokenizer(x['prediction']),\n",
    "                        batched=True\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2766dc6-8a56-4282-be14-cd93ca59e9b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answer', 'prediction', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1452\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78deab94-fff7-4581-a1a5-055c8909a25e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': '배민라이더스는 배민셀프서비스를 통한 배달지역 수정이 가능한가요?',\n",
       " 'answer': '아니요, 배민라이더스는 배민셀프서비스를 통한 배달지역 수정이 불가능합니다. 배달지역 조회만 가능하며, 배달지역 수정이 필요한 경우 고객센터를 통해 문의하셔야 합니다.',\n",
       " 'prediction': '배민라이더스는 배민셀프서비스를 통한 배달지역 수정이 가능한가요? [배민 데이터 참조] 아니요, 배민라이더스는 배민셀프서비스를 통한 배달지역 수정이 불가능합니다. 배달지역 조회만 가능하며, 배달지역 수정이 필요한 경우 고객센터를 통해 문의하셔야 합니다.',\n",
       " 'input_ids': [128000,\n",
       "  103588,\n",
       "  101607,\n",
       "  108157,\n",
       "  102913,\n",
       "  119524,\n",
       "  74769,\n",
       "  101607,\n",
       "  116512,\n",
       "  101204,\n",
       "  125935,\n",
       "  18918,\n",
       "  102681,\n",
       "  24486,\n",
       "  74769,\n",
       "  104684,\n",
       "  120257,\n",
       "  89613,\n",
       "  13094,\n",
       "  125502,\n",
       "  122665,\n",
       "  30,\n",
       "  510,\n",
       "  103588,\n",
       "  101607,\n",
       "  55348,\n",
       "  103718,\n",
       "  93917,\n",
       "  60,\n",
       "  104231,\n",
       "  36811,\n",
       "  11,\n",
       "  74769,\n",
       "  101607,\n",
       "  108157,\n",
       "  102913,\n",
       "  119524,\n",
       "  74769,\n",
       "  101607,\n",
       "  116512,\n",
       "  101204,\n",
       "  125935,\n",
       "  18918,\n",
       "  102681,\n",
       "  24486,\n",
       "  74769,\n",
       "  104684,\n",
       "  120257,\n",
       "  89613,\n",
       "  13094,\n",
       "  102786,\n",
       "  116669,\n",
       "  61938,\n",
       "  13,\n",
       "  74769,\n",
       "  104684,\n",
       "  120257,\n",
       "  98267,\n",
       "  73653,\n",
       "  96451,\n",
       "  108859,\n",
       "  11,\n",
       "  74769,\n",
       "  104684,\n",
       "  120257,\n",
       "  89613,\n",
       "  13094,\n",
       "  126168,\n",
       "  50152,\n",
       "  116534,\n",
       "  110816,\n",
       "  18918,\n",
       "  110155,\n",
       "  108515,\n",
       "  16582,\n",
       "  110311,\n",
       "  90759,\n",
       "  109670,\n",
       "  13],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_dataset['train'][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c9a0e36-1f59-4c84-a00e-a4c866463c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "train_args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=30,\n",
    "\n",
    "        warmup_steps=100,\n",
    "        max_steps=60,\n",
    "\n",
    "        learning_rate=2e-4,\n",
    "\n",
    "        # -- peft -- #\n",
    "        gradient_accumulation_steps=6,\n",
    "        fp16=True,\n",
    "        # ---------- #\n",
    "\n",
    "        logging_steps=20,\n",
    "        output_dir='outputs'\n",
    "    )\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "model=model.to(\"cuda\")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=peft_dataset['train'],\n",
    "    args=train_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "618957a1-0c32-4b32-ae3b-fc3d90bc1cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/freenak/miniconda3/envs/peft_env/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 23:17, Epoch 6/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.305300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.323900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=60, training_loss=2.8497301737467446, metrics={'train_runtime': 1423.7206, 'train_samples_per_second': 7.586, 'train_steps_per_second': 0.042, 'total_flos': 2.2523741514141696e+16, 'train_loss': 2.8497301737467446, 'epoch': 6.73469387755102})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ece613c-4b6f-4d2c-b70b-9e087d42509f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'llama_peft'  # it will be directory\n",
    "trainer.model.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560f6b50-69cd-4756-9d3b-64132c1596fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig.from_pretrained(model_path)\n",
    "model = get_peft_model(model, lora_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
